#!/usr/bin/env bash
# commitments/extract.sh - å¾æ–‡ä»¶ä¸­èƒå–æ”¿ç­–æ‰¿è«¾
#
# åŠŸèƒ½ï¼š
#   1. è®€å– JSONL æ ¼å¼çš„è³‡æ–™
#   2. ä½¿ç”¨ AI è­˜åˆ¥ä¸¦èƒå–æ”¿ç­–æ‰¿è«¾
#   3. ç”¢ç”Ÿ Markdown æª”æ¡ˆåˆ° docs/commitments/
#
# ä½¿ç”¨æ–¹å¼ï¼š
#   ./commitments/extract.sh --input data/daily/2026-01-24.jsonl
#
# ç’°å¢ƒè®Šæ•¸ï¼š
#   OPENAI_API_KEY: OpenAI API key

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
ROOT_DIR="$(cd "${SCRIPT_DIR}/.." && pwd)"

# è¼‰å…¥æ¨¡çµ„
source "${ROOT_DIR}/lib/core.sh"
source "${ROOT_DIR}/lib/args.sh"
source "${ROOT_DIR}/lib/openai.sh"

# è¼¸å‡ºç›®éŒ„
COMMITMENTS_DIR="${ROOT_DIR}/docs/commitments"

########################################
# æ‰¿è«¾èƒå– Prompt
########################################

EXTRACT_SYSTEM_PROMPT='ä½ æ˜¯ä¸€å€‹å°ˆé–€åˆ†æå°ç£æ”¿åºœæ”¿ç­–æ–‡ä»¶çš„åŠ©ç†ã€‚ä½ çš„ä»»å‹™æ˜¯å¾è¡Œæ”¿é™¢å°ç«‹æ³•é™¢çš„ç­”å¾©æ–‡ä»¶ä¸­ï¼Œè­˜åˆ¥ä¸¦èƒå–å…·é«”çš„æ”¿ç­–æ‰¿è«¾ã€‚

æ”¿ç­–æ‰¿è«¾çš„å®šç¾©ï¼š
- æ”¿åºœæ˜ç¢ºè¡¨ç¤ºå°‡è¦é”æˆçš„ç›®æ¨™
- æœ‰å…·é«”æ•¸å­—ã€æ™‚ç¨‹æˆ–å¯è¡¡é‡æŒ‡æ¨™
- æ‰¿è«¾åŸ·è¡Œç‰¹å®šæ”¿ç­–æˆ–æªæ–½

è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
{
  "commitments": [
    {
      "title": "æ‰¿è«¾æ¨™é¡Œï¼ˆç°¡çŸ­æè¿°ï¼Œ20å­—å…§ï¼‰",
      "short_name": "æª”åç”¨ï¼ˆ20å­—å…§ï¼Œåªèƒ½ç”¨ä¸­æ–‡ã€æ•¸å­—ã€é€£å­—è™Ÿï¼Œä¾‹å¦‚ï¼š2025-å†ç”Ÿèƒ½æº20%ï¼‰",
      "category": "åˆ†é¡ï¼ˆèƒ½æºæ”¿ç­–ã€ç’°å¢ƒä¿è­·ã€ç¶“æ¿Ÿç™¼å±•ã€ç¤¾æœƒç¦åˆ©ã€æ•™è‚²ã€äº¤é€šå»ºè¨­ã€é†«ç™‚è¡›ç”Ÿã€åœ‹é˜²å¤–äº¤ã€å…¶ä»–ï¼‰",
      "text": "æ‰¿è«¾çš„åŸæ–‡æ‘˜éŒ„ï¼ˆä¿æŒåŸæ–‡ï¼Œæœ€å¤š200å­—ï¼‰",
      "target_date": "ç›®æ¨™æ—¥æœŸï¼ˆYYYY-MM-DD æ ¼å¼ï¼Œè‹¥åªæœ‰å¹´ä»½å‰‡ç”¨ YYYY-12-31ï¼Œè‹¥ç„¡å‰‡ç‚º nullï¼‰",
      "target_value": "ç›®æ¨™æ•¸å€¼ï¼ˆå¦‚ã€Œ20%ã€ã€ã€Œ5.6GWã€ç­‰ï¼Œè‹¥ç„¡å‰‡ç‚º nullï¼‰",
      "responsible_agency": "è² è²¬æ©Ÿé—œï¼ˆè‹¥æ–‡ä¸­æœ‰æåŠï¼Œå¦å‰‡ç‚º nullï¼‰"
    }
  ]
}

æ³¨æ„äº‹é …ï¼š
1. åªèƒå–æ˜ç¢ºçš„æ‰¿è«¾ï¼Œä¸è¦åŒ…å«æ¨¡ç³Šçš„é¡˜æ™¯é™³è¿°
2. è‹¥æ–‡ä»¶ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ‰¿è«¾ï¼Œå›å‚³ {"commitments": []}
3. æ¯å€‹æ‰¿è«¾æ‡‰è©²æ˜¯ç¨ç«‹ã€å…·é«”çš„é …ç›®
4. short_name ç”¨æ–¼æª”åï¼Œå¿…é ˆç°¡æ½”ä¸”å”¯ä¸€'

########################################
# å‡½å¼
########################################

# ç”¢ç”Ÿæ‰¿è«¾ ID
generate_commitment_id() {
  local text="$1"
  local hash
  if command -v md5sum >/dev/null 2>&1; then
    hash="$(printf '%s' "$text" | md5sum | awk '{print $1}')"
  elif command -v md5 >/dev/null 2>&1; then
    hash="$(printf '%s' "$text" | md5)"
  fi
  printf '%s-%s-%s-%s-%s\n' \
    "${hash:0:8}" "${hash:8:4}" "${hash:12:4}" "${hash:16:4}" "${hash:20:12}"
}

# æª¢æŸ¥æ‰¿è«¾æ˜¯å¦å·²å­˜åœ¨
commitment_exists() {
  local title="$1"
  local index_file="${COMMITMENTS_DIR}/index.json"

  if [[ ! -f "$index_file" ]]; then
    return 1
  fi

  jq -e --arg title "$title" '.categories[].commitments[] | select(.title == $title)' "$index_file" >/dev/null 2>&1
}

# å»ºç«‹ Markdown æª”æ¡ˆ
create_commitment_md() {
  local id="$1"
  local title="$2"
  local short_name="$3"
  local category="$4"
  local text="$5"
  local target_date="$6"
  local target_value="$7"
  local responsible_agency="$8"
  local source_info="$9"

  # ç¢ºä¿åˆ†é¡ç›®éŒ„å­˜åœ¨
  local category_dir="${COMMITMENTS_DIR}/${category}"
  mkdir -p "$category_dir"

  # æª”æ¡ˆè·¯å¾‘
  local file_path="${category_dir}/${short_name}.md"

  # å¦‚æœæª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³é
  if [[ -f "$file_path" ]]; then
    echo "   â­ï¸  æª”æ¡ˆå·²å­˜åœ¨: ${file_path}"
    return 0
  fi

  local today
  today="$(date +%Y-%m-%d)"

  # è§£æä¾†æºè³‡è¨Š
  local src_doc_id src_term src_session src_ey_number src_url
  src_doc_id="$(printf '%s' "$source_info" | jq -r '.document_id')"
  src_term="$(printf '%s' "$source_info" | jq -r '.term')"
  src_session="$(printf '%s' "$source_info" | jq -r '.session_period')"
  src_ey_number="$(printf '%s' "$source_info" | jq -r '.ey_number')"
  src_url="$(printf '%s' "$source_info" | jq -r '.url')"

  # è™•ç† null å€¼
  [[ "$target_date" == "null" || -z "$target_date" ]] && target_date="null" || target_date="\"${target_date}\""
  [[ "$target_value" == "null" || -z "$target_value" ]] && target_value="null" || target_value="\"${target_value}\""
  [[ "$responsible_agency" == "null" || -z "$responsible_agency" ]] && responsible_agency="null" || responsible_agency="\"${responsible_agency}\""

  # ç”¢ç”Ÿ Markdown å…§å®¹
  cat > "$file_path" << EOF
---
id: "${id}"
title: "${title}"
category: "${category}"
status: "è¿½è¹¤ä¸­"
target_date: ${target_date}
target_value: ${target_value}
responsible_agency: ${responsible_agency}
source:
  document_id: "${src_doc_id}"
  term: "${src_term}"
  session_period: "${src_session}"
  ey_number: "${src_ey_number}"
  url: "${src_url}"
created_at: "${today}"
last_updated: "${today}"
---

## æ‰¿è«¾åŸæ–‡

${text}

## è¿½è¹¤ç´€éŒ„

### ${today} [åˆå§‹å»ºç«‹]
å¾ç¬¬${src_term}å±†ç¬¬${src_session}æœƒæœŸç­”å¾©æ–‡ä»¶ä¸­èƒå–æ­¤æ‰¿è«¾ã€‚
EOF

  echo "   âœ… å»ºç«‹: ${file_path}"
  return 0
}

# å¾å–®ä¸€æ–‡ä»¶èƒå–æ‰¿è«¾
extract_from_document() {
  local doc_id="$1"
  local subject="$2"
  local content="$3"
  local source_info="$4"

  # çµ„åˆç”¨æ–¼åˆ†æçš„æ–‡å­—
  local analysis_text="${subject}

${content}"

  # æˆªæ–·éé•·çš„å…§å®¹
  if [[ ${#analysis_text} -gt 8000 ]]; then
    analysis_text="${analysis_text:0:8000}..."
  fi

  # å‘¼å« AI
  local response
  if ! response="$(openai_chat_completion "gpt-4o-mini" "$EXTRACT_SYSTEM_PROMPT" "$analysis_text" "json" 2>&1)"; then
    echo "   âš ï¸  AI å‘¼å«å¤±æ•—: ${response}" >&2
    return 1
  fi

  # é©—è­‰ JSON
  if ! printf '%s' "$response" | jq -e '.commitments' >/dev/null 2>&1; then
    echo "   âš ï¸  å›æ‡‰æ ¼å¼éŒ¯èª¤" >&2
    return 1
  fi

  local count
  count="$(printf '%s' "$response" | jq '.commitments | length')"

  if [[ "$count" -eq 0 ]]; then
    echo "   â„¹ï¸  ç„¡æ‰¿è«¾"
    return 0
  fi

  echo "   âœ… æ‰¾åˆ° ${count} å€‹æ‰¿è«¾"

  # è™•ç†æ¯å€‹æ‰¿è«¾
  while IFS= read -r commitment; do
    local title short_name category text target_date target_value responsible_agency

    title="$(printf '%s' "$commitment" | jq -r '.title')"
    short_name="$(printf '%s' "$commitment" | jq -r '.short_name')"
    category="$(printf '%s' "$commitment" | jq -r '.category')"
    text="$(printf '%s' "$commitment" | jq -r '.text')"
    target_date="$(printf '%s' "$commitment" | jq -r '.target_date // empty')"
    target_value="$(printf '%s' "$commitment" | jq -r '.target_value // empty')"
    responsible_agency="$(printf '%s' "$commitment" | jq -r '.responsible_agency // empty')"

    # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if commitment_exists "$title"; then
      echo "      â­ï¸  æ‰¿è«¾å·²å­˜åœ¨: ${title}"
      continue
    fi

    # ç”¢ç”Ÿ ID
    local id
    id="$(generate_commitment_id "${title}${text}")"

    # å»ºç«‹ Markdown
    create_commitment_md \
      "$id" "$title" "$short_name" "$category" "$text" \
      "$target_date" "$target_value" "$responsible_agency" "$source_info"

    NEW_COUNT=$((NEW_COUNT + 1))

    sleep 0.3
  done < <(printf '%s' "$response" | jq -c '.commitments[]')
}

########################################
# ä¸»ç¨‹å¼
########################################

require_cmd curl jq

parse_args "$@"

arg_required input INPUT_FILE "è¼¸å…¥ JSONL æª”æ¡ˆ"

echo "========================================="
echo "æ”¿ç­–æ‰¿è«¾èƒå–"
echo "========================================="
echo "è¼¸å…¥æª”æ¡ˆ: ${INPUT_FILE}"
echo "è¼¸å‡ºç›®éŒ„: ${COMMITMENTS_DIR}"
echo "========================================="
echo ""

# åˆå§‹åŒ–
openai_init_env || {
  echo "âŒ OpenAI ç’°å¢ƒåˆå§‹åŒ–å¤±æ•—"
  exit 1
}

# ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
mkdir -p "$COMMITMENTS_DIR"

# æª¢æŸ¥è¼¸å…¥æª”æ¡ˆ
if [[ ! -f "$INPUT_FILE" ]]; then
  echo "âŒ è¼¸å…¥æª”æ¡ˆä¸å­˜åœ¨: ${INPUT_FILE}"
  exit 1
fi

# çµ±è¨ˆ
TOTAL_DOCS=0
PROCESSED_DOCS=0
NEW_COUNT=0

# è¨ˆç®—æ–‡ä»¶æ•¸
TOTAL_DOCS="$(jq -s '[.[] | select(.payload.isChunked == false or .payload.chunkIndex == 0)] | length' "$INPUT_FILE")"
echo "ğŸ“Š ç¸½æ–‡ä»¶æ•¸: ${TOTAL_DOCS}"
echo ""

echo "ğŸ”„ é–‹å§‹èƒå–æ‰¿è«¾..."
echo ""

while IFS= read -r line; do
  DOC_ID="$(printf '%s' "$line" | jq -r '.id')"
  SUBJECT="$(printf '%s' "$line" | jq -r '.payload.subject // ""')"
  CONTENT="$(printf '%s' "$line" | jq -r '.payload.content // ""')"
  IS_CHUNKED="$(printf '%s' "$line" | jq -r '.payload.isChunked')"
  CHUNK_INDEX="$(printf '%s' "$line" | jq -r '.payload.chunkIndex // 0')"

  # è·³ééé¦–å€‹ chunk
  if [[ "$IS_CHUNKED" == "true" && "$CHUNK_INDEX" != "0" ]]; then
    continue
  fi

  PROCESSED_DOCS=$((PROCESSED_DOCS + 1))
  echo "[${PROCESSED_DOCS}/${TOTAL_DOCS}] ${SUBJECT:0:50}..."

  # æº–å‚™ä¾†æºè³‡è¨Š
  SOURCE_INFO="$(printf '%s' "$line" | jq -c '{
    document_id: .id,
    term: .payload.term,
    session_period: .payload.sessionPeriod,
    ey_number: .payload.eyNumber,
    url: .payload.docUrl
  }')"

  extract_from_document "$DOC_ID" "$SUBJECT" "$CONTENT" "$SOURCE_INFO"

  sleep 0.5
done < <(jq -c '.' "$INPUT_FILE")

echo ""
echo "========================================="
echo "èƒå–å®Œæˆ"
echo "========================================="
echo "è™•ç†æ–‡ä»¶: ${PROCESSED_DOCS} ç­†"
echo "æ–°å¢æ‰¿è«¾: ${NEW_COUNT} ç­†"
echo "========================================="
