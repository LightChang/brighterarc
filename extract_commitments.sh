#!/usr/bin/env bash
# extract_commitments.sh - å¾ç«‹æ³•é™¢ç­”å¾©è³‡æ–™ä¸­èƒå–æ”¿ç­–æ‰¿è«¾
#
# åŠŸèƒ½ï¼š
#   1. è®€å– JSONL æ ¼å¼çš„ç«‹æ³•é™¢ç­”å¾©è³‡æ–™
#   2. ä½¿ç”¨ OpenAI è­˜åˆ¥ä¸¦èƒå–æ”¿ç­–æ‰¿è«¾
#   3. ç”¢ç”Ÿ embedding ä¸¦ä¸Šå‚³åˆ° Qdrant
#   4. åŒæ™‚å„²å­˜åˆ°æœ¬åœ° JSONL æª”æ¡ˆ
#
# ä½¿ç”¨æ–¹å¼ï¼š
#   ./extract_commitments.sh --input data/daily/2026-01-24.jsonl [--output commitments.jsonl]
#
# ç’°å¢ƒè®Šæ•¸ï¼š
#   OPENAI_API_KEY: OpenAI API key
#   QDRANT_URL: Qdrant ä¼ºæœå™¨ URL
#   QDRANT_API_KEY: Qdrant API key (Cloud ç‰ˆéœ€è¦)

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# è¼‰å…¥æ¨¡çµ„
source "${SCRIPT_DIR}/lib/core.sh"
source "${SCRIPT_DIR}/lib/args.sh"
source "${SCRIPT_DIR}/lib/openai.sh"
source "${SCRIPT_DIR}/lib/qdrant.sh"

# é è¨­ collection åç¨±
COMMITMENT_COLLECTION="policy_commitments"
VECTOR_SIZE=1536

########################################
# æ‰¿è«¾èƒå– Prompt
########################################

COMMITMENT_SYSTEM_PROMPT='ä½ æ˜¯ä¸€å€‹å°ˆé–€åˆ†æå°ç£æ”¿åºœæ”¿ç­–æ–‡ä»¶çš„åŠ©ç†ã€‚ä½ çš„ä»»å‹™æ˜¯å¾è¡Œæ”¿é™¢å°ç«‹æ³•é™¢çš„ç­”å¾©æ–‡ä»¶ä¸­ï¼Œè­˜åˆ¥ä¸¦èƒå–å…·é«”çš„æ”¿ç­–æ‰¿è«¾ã€‚

æ”¿ç­–æ‰¿è«¾çš„å®šç¾©ï¼š
- æ”¿åºœæ˜ç¢ºè¡¨ç¤ºå°‡è¦é”æˆçš„ç›®æ¨™
- æœ‰å…·é«”æ•¸å­—ã€æ™‚ç¨‹æˆ–å¯è¡¡é‡æŒ‡æ¨™
- æ‰¿è«¾åŸ·è¡Œç‰¹å®šæ”¿ç­–æˆ–æªæ–½

è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
{
  "commitments": [
    {
      "text": "æ‰¿è«¾çš„åŸæ–‡æ‘˜éŒ„ï¼ˆä¿æŒåŸæ–‡ï¼Œä½†å¯ç°¡åŒ–å†—é•·éƒ¨åˆ†ï¼‰",
      "target_date": "ç›®æ¨™æ—¥æœŸï¼ˆYYYY-MM-DD æ ¼å¼ï¼Œè‹¥åªæœ‰å¹´ä»½å‰‡ç”¨ YYYY-12-31ï¼Œè‹¥ç„¡å‰‡ç‚º nullï¼‰",
      "target_value": "ç›®æ¨™æ•¸å€¼ï¼ˆå¦‚ã€Œ20%ã€ã€ã€Œ5.6GWã€ç­‰ï¼Œè‹¥ç„¡å‰‡ç‚º nullï¼‰",
      "category": "åˆ†é¡ï¼ˆå¦‚ï¼šèƒ½æºæ”¿ç­–ã€ç’°å¢ƒä¿è­·ã€ç¶“æ¿Ÿç™¼å±•ã€ç¤¾æœƒç¦åˆ©ã€æ•™è‚²ã€äº¤é€šå»ºè¨­ã€é†«ç™‚è¡›ç”Ÿã€åœ‹é˜²å¤–äº¤ã€å…¶ä»–ï¼‰",
      "responsible_agency": "è² è²¬æ©Ÿé—œï¼ˆè‹¥æ–‡ä¸­æœ‰æåŠï¼‰",
      "confidence": "ä¿¡å¿ƒç¨‹åº¦ï¼ˆhigh/medium/lowï¼‰"
    }
  ]
}

æ³¨æ„äº‹é …ï¼š
1. åªèƒå–æ˜ç¢ºçš„æ‰¿è«¾ï¼Œä¸è¦åŒ…å«æ¨¡ç³Šçš„é¡˜æ™¯é™³è¿°
2. è‹¥æ–‡ä»¶ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ‰¿è«¾ï¼Œå›å‚³ {"commitments": []}
3. ä¿æŒ text æ¬„ä½ç°¡æ½”ï¼Œæœ€å¤š 200 å­—
4. æ¯å€‹æ‰¿è«¾æ‡‰è©²æ˜¯ç¨ç«‹ã€å…·é«”çš„é …ç›®'

########################################
# è™•ç†å‡½å¼
########################################

# ç”¢ç”Ÿæ‰¿è«¾ ID
generate_commitment_id() {
  local doc_id="$1"
  local text="$2"

  local unique_string="${doc_id}-${text}"
  local hash
  if command -v md5 >/dev/null 2>&1; then
    hash="$(printf '%s' "$unique_string" | md5)"
  elif command -v md5sum >/dev/null 2>&1; then
    hash="$(printf '%s' "$unique_string" | md5sum | awk '{print $1}')"
  fi

  printf '%s-%s-%s-%s-%s\n' \
    "${hash:0:8}" \
    "${hash:8:4}" \
    "${hash:12:4}" \
    "${hash:16:4}" \
    "${hash:20:12}"
}

# å¾å–®ä¸€æ–‡ä»¶èƒå–æ‰¿è«¾
extract_from_document() {
  local doc_id="$1"
  local subject="$2"
  local content="$3"
  local source_info="$4"

  # çµ„åˆç”¨æ–¼åˆ†æçš„æ–‡å­—ï¼ˆé™åˆ¶é•·åº¦é¿å…è¶…é token é™åˆ¶ï¼‰
  local analysis_text="${subject}

${content}"

  # æˆªæ–·éé•·çš„å…§å®¹ï¼ˆç´„ 8000 å­—å…ƒï¼‰
  if [[ ${#analysis_text} -gt 8000 ]]; then
    analysis_text="${analysis_text:0:8000}..."
  fi

  # å‘¼å« OpenAI API
  local response
  if ! response="$(openai_chat_completion "gpt-4o-mini" "$COMMITMENT_SYSTEM_PROMPT" "$analysis_text" "json" 2>&1)"; then
    echo "âš ï¸  æ–‡ä»¶ ${doc_id} èƒå–å¤±æ•—: ${response}" >&2
    return 1
  fi

  # é©—è­‰ JSON æ ¼å¼
  if ! printf '%s' "$response" | jq -e '.commitments' >/dev/null 2>&1; then
    echo "âš ï¸  æ–‡ä»¶ ${doc_id} å›æ‡‰æ ¼å¼éŒ¯èª¤" >&2
    return 1
  fi

  # å–å¾—æ‰¿è«¾æ•¸é‡
  local commitment_count
  commitment_count="$(printf '%s' "$response" | jq '.commitments | length')"

  if [[ "$commitment_count" -eq 0 ]]; then
    echo "   â„¹ï¸  ç„¡æ‰¿è«¾"
    return 0
  fi

  echo "   âœ… æ‰¾åˆ° ${commitment_count} å€‹æ‰¿è«¾"

  # è™•ç†æ¯å€‹æ‰¿è«¾
  local now
  now="$(date -u +%Y-%m-%dT%H:%M:%SZ)"

  local points_batch="[]"

  while IFS= read -r commitment; do
    local text
    text="$(printf '%s' "$commitment" | jq -r '.text')"

    # ç”¢ç”Ÿå”¯ä¸€ ID
    local commitment_id
    commitment_id="$(generate_commitment_id "$doc_id" "$text")"

    echo "      ğŸ”® ç”¢ç”Ÿ embedding: ${commitment_id:0:8}..."

    # ç”¢ç”Ÿ embedding
    local embedding
    if ! embedding="$(openai_create_embedding "$EMBEDDING_MODEL" "$text" 2>&1)"; then
      echo "      âš ï¸  Embedding å¤±æ•—ï¼Œè·³éæ­¤æ‰¿è«¾" >&2
      continue
    fi

    # çµ„åˆå®Œæ•´çš„æ‰¿è«¾è³‡æ–™
    local full_commitment
    full_commitment="$(printf '%s' "$commitment" | jq -c \
      --arg id "$commitment_id" \
      --arg doc_id "$doc_id" \
      --arg now "$now" \
      --argjson source "$source_info" \
      '. + {
        id: $id,
        source: $source,
        status: "pending",
        extracted_at: $now
      }'
    )"

    # çµ„åˆæˆ Qdrant point æ ¼å¼
    local point
    point="$(printf '%s\n%s' "$embedding" "$full_commitment" | jq -sc \
      --arg id "$commitment_id" \
      '{
        id: $id,
        vector: .[0],
        payload: .[1]
      }'
    )"

    # åŠ å…¥æ‰¹æ¬¡
    points_batch="$(printf '%s\n%s' "$points_batch" "$point" | jq -sc '.[0] + [.[1]]')"

    # å¯«å…¥æœ¬åœ°æª”æ¡ˆ
    echo "$full_commitment" >> "$OUTPUT_FILE"

    COMMITMENT_COUNT=$((COMMITMENT_COUNT + 1))

    # API rate limit ä¿è­·
    sleep 0.3

  done < <(printf '%s' "$response" | jq -c '.commitments[]')

  # æ‰¹æ¬¡ä¸Šå‚³åˆ° Qdrant
  local batch_size
  batch_size="$(printf '%s' "$points_batch" | jq 'length')"

  if [[ "$batch_size" -gt 0 ]]; then
    echo "      ğŸ“¤ ä¸Šå‚³ ${batch_size} å€‹æ‰¿è«¾åˆ° Qdrant..."
    if qdrant_upsert_points_batch "$COMMITMENT_COLLECTION" "$points_batch"; then
      echo "      âœ… ä¸Šå‚³æˆåŠŸ"
    else
      echo "      âš ï¸  ä¸Šå‚³å¤±æ•—ï¼ˆè³‡æ–™å·²å„²å­˜åˆ°æœ¬åœ°æª”æ¡ˆï¼‰" >&2
    fi
  fi
}

########################################
# ä¸»ç¨‹å¼
########################################

# æª¢æŸ¥å¿…è¦æŒ‡ä»¤
require_cmd curl jq

# è§£æåƒæ•¸
parse_args "$@"

arg_required input INPUT_FILE
arg_optional output OUTPUT_FILE ""
arg_optional limit PROCESS_LIMIT "0"
arg_optional model EMBEDDING_MODEL "text-embedding-3-small"
arg_optional collection COMMITMENT_COLLECTION "policy_commitments"

# è‹¥æœªæŒ‡å®šè¼¸å‡ºæª”æ¡ˆï¼Œè‡ªå‹•ç”¢ç”Ÿ
if [[ -z "$OUTPUT_FILE" ]]; then
  INPUT_BASENAME="$(basename "$INPUT_FILE" .jsonl)"
  OUTPUT_FILE="data/commitments/${INPUT_BASENAME}-commitments.jsonl"
fi

echo "========================================="
echo "æ”¿ç­–æ‰¿è«¾èƒå–å¼•æ“"
echo "========================================="
echo "è¼¸å…¥æª”æ¡ˆ: ${INPUT_FILE}"
echo "è¼¸å‡ºæª”æ¡ˆ: ${OUTPUT_FILE}"
echo "Qdrant Collection: ${COMMITMENT_COLLECTION}"
echo "è™•ç†é™åˆ¶: ${PROCESS_LIMIT:-ç„¡é™åˆ¶}"
echo "Embedding Model: ${EMBEDDING_MODEL}"
echo "========================================="
echo ""

# åˆå§‹åŒ–ç’°å¢ƒ
echo "ğŸ”§ åˆå§‹åŒ–ç’°å¢ƒ..."

openai_init_env || {
  echo "âŒ OpenAI ç’°å¢ƒåˆå§‹åŒ–å¤±æ•—"
  exit 1
}

qdrant_init_env || {
  echo "âŒ Qdrant ç’°å¢ƒåˆå§‹åŒ–å¤±æ•—"
  exit 1
}

echo "âœ… ç’°å¢ƒåˆå§‹åŒ–å®Œæˆ"
echo ""

# æª¢æŸ¥è¼¸å…¥æª”æ¡ˆ
if [[ ! -f "$INPUT_FILE" ]]; then
  echo "âŒ è¼¸å…¥æª”æ¡ˆä¸å­˜åœ¨: ${INPUT_FILE}"
  exit 1
fi

# ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
OUTPUT_DIR="$(dirname "$OUTPUT_FILE")"
mkdir -p "$OUTPUT_DIR"

# ç¢ºä¿ Qdrant collection å­˜åœ¨
echo "ğŸ”§ ç¢ºèª Qdrant collection..."
if ! qdrant_collection_exists "$COMMITMENT_COLLECTION"; then
  echo "   ğŸ“¦ å»ºç«‹ collection: ${COMMITMENT_COLLECTION}"
  qdrant_create_collection "$COMMITMENT_COLLECTION" "$VECTOR_SIZE" "Cosine" || {
    echo "âŒ ç„¡æ³•å»ºç«‹ Qdrant collection"
    exit 1
  }
fi
echo "âœ… Collection å°±ç·’"
echo ""

# çµ±è¨ˆ
TOTAL_DOCS=0
PROCESSED_DOCS=0
COMMITMENT_COUNT=0
ERROR_COUNT=0

# è¨ˆç®—ç¸½æ–‡ä»¶æ•¸ï¼ˆåªè¨ˆç®—é chunk æˆ– chunk 0 çš„æ–‡ä»¶ï¼‰
TOTAL_DOCS="$(jq -s '[.[] | select(.payload.isChunked == false or .payload.chunkIndex == 0)] | length' "$INPUT_FILE")"
echo "ğŸ“Š ç¸½æ–‡ä»¶æ•¸: ${TOTAL_DOCS}"
echo ""

# è™•ç†æ¯å€‹æ–‡ä»¶
echo "ğŸ”„ é–‹å§‹èƒå–æ‰¿è«¾..."
echo ""

while IFS= read -r line; do
  # æª¢æŸ¥è™•ç†é™åˆ¶
  if [[ "$PROCESS_LIMIT" -gt 0 && "$PROCESSED_DOCS" -ge "$PROCESS_LIMIT" ]]; then
    echo ""
    echo "âœ… å·²é”è™•ç†é™åˆ¶ ${PROCESS_LIMIT} ç­†"
    break
  fi

  # æå–æ–‡ä»¶è³‡è¨Š
  DOC_ID="$(printf '%s' "$line" | jq -r '.id')"
  SUBJECT="$(printf '%s' "$line" | jq -r '.payload.subject // ""')"
  CONTENT="$(printf '%s' "$line" | jq -r '.payload.content // ""')"
  IS_CHUNKED="$(printf '%s' "$line" | jq -r '.payload.isChunked')"
  CHUNK_INDEX="$(printf '%s' "$line" | jq -r '.payload.chunkIndex // 0')"

  # è·³ééé¦–å€‹ chunk çš„æ–‡ä»¶ï¼ˆé¿å…é‡è¤‡è™•ç†ï¼‰
  if [[ "$IS_CHUNKED" == "true" && "$CHUNK_INDEX" != "0" ]]; then
    continue
  fi

  # å¦‚æœæ˜¯ chunked æ–‡ä»¶ï¼Œæ”¶é›†æ‰€æœ‰ chunks çš„å…§å®¹
  if [[ "$IS_CHUNKED" == "true" ]]; then
    BASE_ID="$(printf '%s' "$line" | jq -r '.payload.baseId')"
    # åˆä½µæ‰€æœ‰ chunks çš„ chunkText
    CONTENT="$(jq -r --arg base_id "$BASE_ID" '
      select(.payload.baseId == $base_id) |
      .payload.chunkText // .payload.content
    ' "$INPUT_FILE" | tr '\n' ' ')"
  fi

  PROCESSED_DOCS=$((PROCESSED_DOCS + 1))
  echo "[${PROCESSED_DOCS}/${TOTAL_DOCS}] ${DOC_ID:0:8}... ${SUBJECT:0:40}"

  # æº–å‚™ä¾†æºè³‡è¨Š
  SOURCE_INFO="$(printf '%s' "$line" | jq -c '{
    document_id: .id,
    term: .payload.term,
    sessionPeriod: .payload.sessionPeriod,
    sessionTimes: .payload.sessionTimes,
    eyNumber: .payload.eyNumber,
    lyNumber: .payload.lyNumber,
    subject: .payload.subject
  }')"

  # èƒå–æ‰¿è«¾
  if ! extract_from_document "$DOC_ID" "$SUBJECT" "$CONTENT" "$SOURCE_INFO"; then
    ERROR_COUNT=$((ERROR_COUNT + 1))
  fi

  # API rate limit ä¿è­·
  sleep 0.5

done < <(jq -c '.' "$INPUT_FILE")

echo ""
echo "========================================="
echo "èƒå–å®Œæˆ"
echo "========================================="
echo "è™•ç†æ–‡ä»¶: ${PROCESSED_DOCS} ç­†"
echo "èƒå–æ‰¿è«¾: ${COMMITMENT_COUNT} ç­†"
echo "å¤±æ•—æ–‡ä»¶: ${ERROR_COUNT} ç­†"
echo "è¼¸å‡ºæª”æ¡ˆ: ${OUTPUT_FILE}"
echo "Qdrant Collection: ${COMMITMENT_COLLECTION}"
echo "========================================="

# é¡¯ç¤ºæ‰¿è«¾çµ±è¨ˆ
if [[ -f "$OUTPUT_FILE" && -s "$OUTPUT_FILE" ]]; then
  echo ""
  echo "ğŸ“Š æ‰¿è«¾åˆ†é¡çµ±è¨ˆï¼š"
  jq -r '.category' "$OUTPUT_FILE" | sort | uniq -c | sort -rn
fi
