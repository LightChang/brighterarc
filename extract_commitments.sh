#!/usr/bin/env bash
# extract_commitments.sh - å¾ç«‹æ³•é™¢ç­”å¾©è³‡æ–™ä¸­èƒå–æ”¿ç­–æ‰¿è«¾
#
# åŠŸèƒ½ï¼š
#   1. è®€å– JSONL æ ¼å¼çš„ç«‹æ³•é™¢ç­”å¾©è³‡æ–™
#   2. ä½¿ç”¨ OpenAI è­˜åˆ¥ä¸¦èƒå–æ”¿ç­–æ‰¿è«¾
#   3. è¼¸å‡ºçµæ§‹åŒ–çš„æ‰¿è«¾è³‡æ–™
#
# ä½¿ç”¨æ–¹å¼ï¼š
#   ./extract_commitments.sh --input data/daily/2026-01-24.jsonl [--output commitments.jsonl]
#
# ç’°å¢ƒè®Šæ•¸ï¼š
#   OPENAI_API_KEY: OpenAI API key

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# è¼‰å…¥æ¨¡çµ„
source "${SCRIPT_DIR}/lib/core.sh"
source "${SCRIPT_DIR}/lib/args.sh"
source "${SCRIPT_DIR}/lib/openai.sh"

########################################
# æ‰¿è«¾èƒå– Prompt
########################################

COMMITMENT_SYSTEM_PROMPT='ä½ æ˜¯ä¸€å€‹å°ˆé–€åˆ†æå°ç£æ”¿åºœæ”¿ç­–æ–‡ä»¶çš„åŠ©ç†ã€‚ä½ çš„ä»»å‹™æ˜¯å¾è¡Œæ”¿é™¢å°ç«‹æ³•é™¢çš„ç­”å¾©æ–‡ä»¶ä¸­ï¼Œè­˜åˆ¥ä¸¦èƒå–å…·é«”çš„æ”¿ç­–æ‰¿è«¾ã€‚

æ”¿ç­–æ‰¿è«¾çš„å®šç¾©ï¼š
- æ”¿åºœæ˜ç¢ºè¡¨ç¤ºå°‡è¦é”æˆçš„ç›®æ¨™
- æœ‰å…·é«”æ•¸å­—ã€æ™‚ç¨‹æˆ–å¯è¡¡é‡æŒ‡æ¨™
- æ‰¿è«¾åŸ·è¡Œç‰¹å®šæ”¿ç­–æˆ–æªæ–½

è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
{
  "commitments": [
    {
      "text": "æ‰¿è«¾çš„åŸæ–‡æ‘˜éŒ„ï¼ˆä¿æŒåŸæ–‡ï¼Œä½†å¯ç°¡åŒ–å†—é•·éƒ¨åˆ†ï¼‰",
      "target_date": "ç›®æ¨™æ—¥æœŸï¼ˆYYYY-MM-DD æ ¼å¼ï¼Œè‹¥åªæœ‰å¹´ä»½å‰‡ç”¨ YYYY-12-31ï¼Œè‹¥ç„¡å‰‡ç‚º nullï¼‰",
      "target_value": "ç›®æ¨™æ•¸å€¼ï¼ˆå¦‚ã€Œ20%ã€ã€ã€Œ5.6GWã€ç­‰ï¼Œè‹¥ç„¡å‰‡ç‚º nullï¼‰",
      "category": "åˆ†é¡ï¼ˆå¦‚ï¼šèƒ½æºæ”¿ç­–ã€ç’°å¢ƒä¿è­·ã€ç¶“æ¿Ÿç™¼å±•ã€ç¤¾æœƒç¦åˆ©ã€æ•™è‚²ã€äº¤é€šå»ºè¨­ã€é†«ç™‚è¡›ç”Ÿã€åœ‹é˜²å¤–äº¤ã€å…¶ä»–ï¼‰",
      "responsible_agency": "è² è²¬æ©Ÿé—œï¼ˆè‹¥æ–‡ä¸­æœ‰æåŠï¼‰",
      "confidence": "ä¿¡å¿ƒç¨‹åº¦ï¼ˆhigh/medium/lowï¼‰"
    }
  ]
}

æ³¨æ„äº‹é …ï¼š
1. åªèƒå–æ˜ç¢ºçš„æ‰¿è«¾ï¼Œä¸è¦åŒ…å«æ¨¡ç³Šçš„é¡˜æ™¯é™³è¿°
2. è‹¥æ–‡ä»¶ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½•æ‰¿è«¾ï¼Œå›å‚³ {"commitments": []}
3. ä¿æŒ text æ¬„ä½ç°¡æ½”ï¼Œæœ€å¤š 200 å­—
4. æ¯å€‹æ‰¿è«¾æ‡‰è©²æ˜¯ç¨ç«‹ã€å…·é«”çš„é …ç›®'

########################################
# è™•ç†å‡½å¼
########################################

# å¾å–®ä¸€æ–‡ä»¶èƒå–æ‰¿è«¾
extract_from_document() {
  local doc_id="$1"
  local subject="$2"
  local content="$3"
  local source_info="$4"

  # çµ„åˆç”¨æ–¼åˆ†æçš„æ–‡å­—ï¼ˆé™åˆ¶é•·åº¦é¿å…è¶…é token é™åˆ¶ï¼‰
  local analysis_text="${subject}

${content}"

  # æˆªæ–·éé•·çš„å…§å®¹ï¼ˆç´„ 8000 å­—å…ƒï¼‰
  if [[ ${#analysis_text} -gt 8000 ]]; then
    analysis_text="${analysis_text:0:8000}..."
  fi

  # å‘¼å« OpenAI API
  local response
  if ! response="$(openai_chat_completion "gpt-4o-mini" "$COMMITMENT_SYSTEM_PROMPT" "$analysis_text" "json" 2>&1)"; then
    echo "âš ï¸  æ–‡ä»¶ ${doc_id} èƒå–å¤±æ•—: ${response}" >&2
    return 1
  fi

  # é©—è­‰ JSON æ ¼å¼
  if ! printf '%s' "$response" | jq -e '.commitments' >/dev/null 2>&1; then
    echo "âš ï¸  æ–‡ä»¶ ${doc_id} å›æ‡‰æ ¼å¼éŒ¯èª¤" >&2
    return 1
  fi

  # å–å¾—æ‰¿è«¾æ•¸é‡
  local commitment_count
  commitment_count="$(printf '%s' "$response" | jq '.commitments | length')"

  if [[ "$commitment_count" -eq 0 ]]; then
    echo "   â„¹ï¸  ç„¡æ‰¿è«¾"
    return 0
  fi

  echo "   âœ… æ‰¾åˆ° ${commitment_count} å€‹æ‰¿è«¾"

  # ç‚ºæ¯å€‹æ‰¿è«¾åŠ ä¸Šä¾†æºè³‡è¨Šå’Œ ID
  local now
  now="$(date -u +%Y-%m-%dT%H:%M:%SZ)"

  printf '%s' "$response" | jq -c --arg source_info "$source_info" --arg doc_id "$doc_id" --arg now "$now" '
    .commitments[] |
    . + {
      id: (($doc_id) + "-" + (. | @base64 | .[0:8])),
      source: ($source_info | fromjson),
      status: "pending",
      extracted_at: $now
    }
  '
}

########################################
# ä¸»ç¨‹å¼
########################################

# æª¢æŸ¥å¿…è¦æŒ‡ä»¤
require_cmd curl jq

# è§£æåƒæ•¸
parse_args "$@"

arg_required input INPUT_FILE
arg_optional output OUTPUT_FILE ""
arg_optional limit PROCESS_LIMIT "0"
arg_optional model CHAT_MODEL "gpt-4o-mini"

# è‹¥æœªæŒ‡å®šè¼¸å‡ºæª”æ¡ˆï¼Œè‡ªå‹•ç”¢ç”Ÿ
if [[ -z "$OUTPUT_FILE" ]]; then
  INPUT_BASENAME="$(basename "$INPUT_FILE" .jsonl)"
  OUTPUT_FILE="data/commitments/${INPUT_BASENAME}-commitments.jsonl"
fi

echo "========================================="
echo "æ”¿ç­–æ‰¿è«¾èƒå–å¼•æ“"
echo "========================================="
echo "è¼¸å…¥æª”æ¡ˆ: ${INPUT_FILE}"
echo "è¼¸å‡ºæª”æ¡ˆ: ${OUTPUT_FILE}"
echo "è™•ç†é™åˆ¶: ${PROCESS_LIMIT:-ç„¡é™åˆ¶}"
echo "ä½¿ç”¨æ¨¡å‹: ${CHAT_MODEL}"
echo "========================================="
echo ""

# åˆå§‹åŒ– OpenAI
echo "ğŸ”§ åˆå§‹åŒ–ç’°å¢ƒ..."
openai_init_env || {
  echo "âŒ OpenAI ç’°å¢ƒåˆå§‹åŒ–å¤±æ•—"
  exit 1
}
echo "âœ… ç’°å¢ƒåˆå§‹åŒ–å®Œæˆ"
echo ""

# æª¢æŸ¥è¼¸å…¥æª”æ¡ˆ
if [[ ! -f "$INPUT_FILE" ]]; then
  echo "âŒ è¼¸å…¥æª”æ¡ˆä¸å­˜åœ¨: ${INPUT_FILE}"
  exit 1
fi

# ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
OUTPUT_DIR="$(dirname "$OUTPUT_FILE")"
mkdir -p "$OUTPUT_DIR"

# çµ±è¨ˆ
TOTAL_DOCS=0
PROCESSED_DOCS=0
COMMITMENT_COUNT=0
ERROR_COUNT=0

# è¨ˆç®—ç¸½æ–‡ä»¶æ•¸ï¼ˆåªè¨ˆç®—é chunk æˆ– chunk 0 çš„æ–‡ä»¶ï¼‰
TOTAL_DOCS="$(jq -s '[.[] | select(.payload.isChunked == false or .payload.chunkIndex == 0)] | length' "$INPUT_FILE")"
echo "ğŸ“Š ç¸½æ–‡ä»¶æ•¸: ${TOTAL_DOCS}"
echo ""

# è™•ç†æ¯å€‹æ–‡ä»¶
echo "ğŸ”„ é–‹å§‹èƒå–æ‰¿è«¾..."
echo ""

while IFS= read -r line; do
  # æª¢æŸ¥è™•ç†é™åˆ¶
  if [[ "$PROCESS_LIMIT" -gt 0 && "$PROCESSED_DOCS" -ge "$PROCESS_LIMIT" ]]; then
    echo ""
    echo "âœ… å·²é”è™•ç†é™åˆ¶ ${PROCESS_LIMIT} ç­†"
    break
  fi

  # æå–æ–‡ä»¶è³‡è¨Š
  DOC_ID="$(printf '%s' "$line" | jq -r '.id')"
  SUBJECT="$(printf '%s' "$line" | jq -r '.payload.subject // ""')"
  CONTENT="$(printf '%s' "$line" | jq -r '.payload.content // ""')"
  IS_CHUNKED="$(printf '%s' "$line" | jq -r '.payload.isChunked')"
  CHUNK_INDEX="$(printf '%s' "$line" | jq -r '.payload.chunkIndex // 0')"

  # è·³ééé¦–å€‹ chunk çš„æ–‡ä»¶ï¼ˆé¿å…é‡è¤‡è™•ç†ï¼‰
  if [[ "$IS_CHUNKED" == "true" && "$CHUNK_INDEX" != "0" ]]; then
    continue
  fi

  # å¦‚æœæ˜¯ chunked æ–‡ä»¶ï¼Œæ”¶é›†æ‰€æœ‰ chunks çš„å…§å®¹
  if [[ "$IS_CHUNKED" == "true" ]]; then
    BASE_ID="$(printf '%s' "$line" | jq -r '.payload.baseId')"
    # åˆä½µæ‰€æœ‰ chunks çš„ chunkText
    CONTENT="$(jq -r --arg base_id "$BASE_ID" '
      select(.payload.baseId == $base_id) |
      .payload.chunkText // .payload.content
    ' "$INPUT_FILE" | tr '\n' ' ')"
  fi

  PROCESSED_DOCS=$((PROCESSED_DOCS + 1))
  echo "[${PROCESSED_DOCS}/${TOTAL_DOCS}] ${DOC_ID:0:8}... ${SUBJECT:0:40}"

  # æº–å‚™ä¾†æºè³‡è¨Š
  SOURCE_INFO="$(printf '%s' "$line" | jq -c '{
    document_id: .id,
    term: .payload.term,
    sessionPeriod: .payload.sessionPeriod,
    sessionTimes: .payload.sessionTimes,
    eyNumber: .payload.eyNumber,
    lyNumber: .payload.lyNumber,
    subject: .payload.subject
  }')"

  # èƒå–æ‰¿è«¾
  if commitments="$(extract_from_document "$DOC_ID" "$SUBJECT" "$CONTENT" "$SOURCE_INFO")"; then
    if [[ -n "$commitments" ]]; then
      # å¯«å…¥è¼¸å‡ºæª”æ¡ˆ
      printf '%s\n' "$commitments" >> "$OUTPUT_FILE"
      # è¨ˆç®—æœ¬æ¬¡æ–°å¢çš„æ‰¿è«¾æ•¸
      new_count="$(printf '%s' "$commitments" | wc -l | tr -d ' ')"
      COMMITMENT_COUNT=$((COMMITMENT_COUNT + new_count))
    fi
  else
    ERROR_COUNT=$((ERROR_COUNT + 1))
  fi

  # API rate limit ä¿è­·
  sleep 0.5

done < <(jq -c '.' "$INPUT_FILE")

echo ""
echo "========================================="
echo "èƒå–å®Œæˆ"
echo "========================================="
echo "è™•ç†æ–‡ä»¶: ${PROCESSED_DOCS} ç­†"
echo "èƒå–æ‰¿è«¾: ${COMMITMENT_COUNT} ç­†"
echo "å¤±æ•—æ–‡ä»¶: ${ERROR_COUNT} ç­†"
echo "è¼¸å‡ºæª”æ¡ˆ: ${OUTPUT_FILE}"
echo "========================================="

# é¡¯ç¤ºæ‰¿è«¾çµ±è¨ˆ
if [[ -f "$OUTPUT_FILE" && -s "$OUTPUT_FILE" ]]; then
  echo ""
  echo "ğŸ“Š æ‰¿è«¾åˆ†é¡çµ±è¨ˆï¼š"
  jq -r '.category' "$OUTPUT_FILE" | sort | uniq -c | sort -rn
fi
